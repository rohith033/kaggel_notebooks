{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":7294503,"sourceType":"datasetVersion","datasetId":4210720}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft\nimport torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom peft import get_peft_model\nfrom peft import LoraConfig, TaskType\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments","metadata":{"_uuid":"323a62a5-b318-4654-8131-f27137a968af","_cell_guid":"238df9b4-ff03-4903-bdb5-d46f59757e51","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-31T10:19:11.914903Z","iopub.execute_input":"2023-12-31T10:19:11.915798Z","iopub.status.idle":"2023-12-31T10:19:23.831404Z","shell.execute_reply.started":"2023-12-31T10:19:11.915762Z","shell.execute_reply":"2023-12-31T10:19:23.830176Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.36.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.19.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"competitionData = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\nexternalData = pd.read_csv('/kaggle/input/daigt-proper-train-dataset/train_drcat_01.csv')\ncompetitionData = competitionData.drop(['id', 'prompt_id'], axis=1)\nexternalData = externalData.drop(['source', 'fold'], axis=1)\ncompetitionData = competitionData.rename(columns={'generated': 'label'})\nfinalData = pd.concat([competitionData, externalData])\n\n# Load the pre-trained BERT model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Split the data into training and test sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    finalData['text'].tolist(),\n    finalData['label'].tolist(),\n    test_size=0.2,\n    random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:19:23.834212Z","iopub.execute_input":"2023-12-31T10:19:23.834672Z","iopub.status.idle":"2023-12-31T10:19:25.378896Z","shell.execute_reply.started":"2023-12-31T10:19:23.834624Z","shell.execute_reply":"2023-12-31T10:19:25.377832Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize the training and test sets\ntokenized_train = tokenizer(\n    train_texts,\n    padding=True,\n    truncation=True,\n    return_tensors='pt'\n)\n\ntokenized_test = tokenizer(\n    test_texts,\n    padding=True,\n    truncation=True,\n    return_tensors='pt'\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:19:25.383602Z","iopub.execute_input":"2023-12-31T10:19:25.383888Z","iopub.status.idle":"2023-12-31T10:20:03.599748Z","shell.execute_reply.started":"2023-12-31T10:19:25.383863Z","shell.execute_reply":"2023-12-31T10:20:03.598581Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create PyTorch datasets for training and test sets\nclass CustomDataset(Dataset):\n    def __init__(self, input_ids, attention_mask, labels):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.input_ids[idx],\n            'attention_mask': self.attention_mask[idx],\n            'labels': self.labels[idx]\n        }","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:20:03.603001Z","iopub.execute_input":"2023-12-31T10:20:03.603785Z","iopub.status.idle":"2023-12-31T10:20:03.612407Z","shell.execute_reply.started":"2023-12-31T10:20:03.603744Z","shell.execute_reply":"2023-12-31T10:20:03.611385Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Training dataset\ntrain_dataset = CustomDataset(\n    input_ids=tokenized_train['input_ids'],\n    attention_mask=tokenized_train['attention_mask'],\n    labels=train_labels\n)\n\n# Test dataset\ntest_dataset = CustomDataset(\n    input_ids=tokenized_test['input_ids'],\n    attention_mask=tokenized_test['attention_mask'],\n    labels=test_labels\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:20:03.613528Z","iopub.execute_input":"2023-12-31T10:20:03.613775Z","iopub.status.idle":"2023-12-31T10:20:03.642341Z","shell.execute_reply.started":"2023-12-31T10:20:03.613753Z","shell.execute_reply":"2023-12-31T10:20:03.640952Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Configure PEFT model\npeft_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:20:03.643713Z","iopub.execute_input":"2023-12-31T10:20:03.644051Z","iopub.status.idle":"2023-12-31T10:20:03.710370Z","shell.execute_reply.started":"2023-12-31T10:20:03.644020Z","shell.execute_reply":"2023-12-31T10:20:03.709361Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    learning_rate=1e-3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:23:21.163882Z","iopub.execute_input":"2023-12-31T10:23:21.164254Z","iopub.status.idle":"2023-12-31T10:23:21.174040Z","shell.execute_reply.started":"2023-12-31T10:23:21.164225Z","shell.execute_reply":"2023-12-31T10:23:21.172946Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# Move model to GPU for training\ntrainer.model.to('cuda')\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T10:23:25.883483Z","iopub.execute_input":"2023-12-31T10:23:25.884096Z","iopub.status.idle":"2023-12-31T11:06:13.829757Z","shell.execute_reply.started":"2023-12-31T10:23:25.884057Z","shell.execute_reply":"2023-12-31T11:06:13.828677Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6928' max='6928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6928/6928 42:46, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.034700</td>\n      <td>0.053801</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.004800</td>\n      <td>0.016471</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6928, training_loss=0.028100146646944775, metrics={'train_runtime': 2567.2739, 'train_samples_per_second': 21.586, 'train_steps_per_second': 2.699, 'total_flos': 1.463155732819968e+16, 'train_loss': 0.028100146646944775, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T11:13:03.122962Z","iopub.execute_input":"2023-12-31T11:13:03.123856Z","iopub.status.idle":"2023-12-31T11:13:03.146372Z","shell.execute_reply.started":"2023-12-31T11:13:03.123820Z","shell.execute_reply":"2023-12-31T11:13:03.145306Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()\nmodel.push_to_hub(\"aiVshuman_bert_2epochs\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T11:17:15.533452Z","iopub.execute_input":"2023-12-31T11:17:15.534219Z","iopub.status.idle":"2023-12-31T11:17:16.961236Z","shell.execute_reply.started":"2023-12-31T11:17:15.534185Z","shell.execute_reply":"2023-12-31T11:17:16.960153Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583004a7d88d487fbbc0045014131dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.19M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19dfafc7068f486da5008116f7991817"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/MonkeyDdonut/aiVshuman_bert_2epochs/commit/4fd51207a9b760b92096f67b89583dcae78aea22', commit_message='Upload model', commit_description='', oid='4fd51207a9b760b92096f67b89583dcae78aea22', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}